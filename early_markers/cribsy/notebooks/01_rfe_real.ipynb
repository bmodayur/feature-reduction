{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "doc-main-summary",
   "metadata": {},
   "source": [
    "# 1 - Real Data: Iterative Feature Selection with Enhanced Adaptive RFE\n",
    "\n",
    "**Purpose**: This notebook performs comprehensive feature selection on real infant movement data using an iterative Enhanced Adaptive Recursive Feature Elimination (RFE) approach. It progressively reduces the feature space while computing Bayesian surprise metrics and ROC performance at each iteration.\n",
    "\n",
    "**Inputs**:\n",
    "- Real movement data loaded from `/Volumes/secure/code/early-markers/early_markers/emmacp_metrics/features_merged.pkl`\n",
    "- 57 baseline movement features (position, velocity, acceleration, entropy, correlation metrics)\n",
    "- Configuration constants from `early_markers.cribsy.common.constants`\n",
    "\n",
    "**Outputs**:\n",
    "- `/Volumes/secure/data/early_markers/cribsy/pkl/bd_real.pkl` - Complete BayesianData object with all models\n",
    "- `/Volumes/secure/data/early_markers/cribsy/xlsx/real_*.xlsx` - Excel reports with metrics and summaries\n",
    "- Console output showing feature progression and final selected features\n",
    "\n",
    "**Key Dependencies**:\n",
    "- `BayesianData`: Core class managing data loading, RFE, surprise computation, and metrics\n",
    "- `EnhancedAdaptiveRFE`: Statistical feature selection with noise injection and consensus testing\n",
    "- Random seed: `RAND_STATE = 20250313` for reproducibility\n",
    "\n",
    "**Workflow Overview**:\n",
    "1. Initialize BayesianData with real movement data\n",
    "2. Iteratively apply Enhanced Adaptive RFE to reduce feature count\n",
    "3. Compute Bayesian surprise metrics after each RFE iteration\n",
    "4. Calculate ROC performance (sensitivity, specificity, AUC)\n",
    "5. Continue until minimum feature threshold (`MIN_K = 7`) is reached\n",
    "6. Export comprehensive results to Excel and pickle formats\n",
    "\n",
    "**Note**: This is a computationally intensive notebook. Full execution takes approximately 20-25 minutes with 50 RFE trials per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc-section-1",
   "metadata": {},
   "source": [
    "## 1.1 - Environment Setup and Data Loading\n",
    "\n",
    "This section initializes the analysis environment:\n",
    "- Sets the global random seed for reproducible results across all operations\n",
    "- Imports core dependencies:\n",
    "  - `BayesianData`: Primary data management and analysis class\n",
    "  - `RAND_STATE`: Global random seed (20250313)\n",
    "  - `MIN_K`: Minimum feature count threshold (7 features)\n",
    "  - `PKL_DIR`: Output directory for pickle files\n",
    "  - `FEATURES`: Complete list of 57 baseline movement features\n",
    "- Initializes `BayesianData()` which automatically:\n",
    "  - Loads `features_merged.pkl` from the raw data directory\n",
    "  - Transforms risk labels (0=normal, 1=at-risk)\n",
    "  - Splits data into training (category=1) and test (category=2) sets\n",
    "  - Prepares both long and wide format DataFrames for analysis\n",
    "\n",
    "The `drops` list contains features previously identified as unstable or redundant but is not applied in this analysis (we use all `FEATURES`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-05T22:23:36.989000Z",
     "start_time": "2025-10-05T22:23:34.854101Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from numpy import random\n",
    "from loguru import logger\n",
    "\n",
    "from early_markers.cribsy.common.bayes import BayesianData\n",
    "from early_markers.cribsy.common.constants import AGE_BRACKETS, MIN_K, PKL_DIR, FEATURES\n",
    "from early_markers.cribsy.common.constants import RAND_STATE\n",
    "\n",
    "# Set seeds at file level for reproducibility\n",
    "random.seed(RAND_STATE)\n",
    "\n",
    "# Initialize BayesianData - loads real data from features_merged.pkl\n",
    "bd = BayesianData()\n",
    "\n",
    "start_time = datetime.now()\n",
    "logger.debug(f\"Starting Feature Selection...\")\n",
    "\n",
    "# Feature list note: 'drops' contains previously identified unstable features\n",
    "# but we use all FEATURES for comprehensive analysis\n",
    "drops = ['Shoulder_IQR_vel_angle', 'Ankle_IQRaccx', 'Wrist_IQRaccx', 'Ankle_IQRvelx', 'Knee_IQR_vel_angle', 'Elbow_IQR_acc_angle', 'Shoulder_mean_angle', 'Ankle_IQRaccy', 'Shoulder_lrCorr_angle', 'Hip_entropy_angle', 'Elbow_mean_angle', 'Eye_lrCorr_x', 'Shoulder_entropy_angle', 'Knee_entropy_angle', 'Shoulder_IQR_acc_angle', 'Ankle_lrCorr_x', 'Hip_lrCorr_angle', 'Wrist_meanent', 'Wrist_IQRvelx', 'Wrist_mediany', 'Ankle_IQRvely', 'Shoulder_stdev_angle', 'Hip_IQR_acc_angle', 'Elbow_stdev_angle', 'Knee_IQR_acc_angle', 'Ankle_meanent', 'Ankle_medianx', 'Wrist_IQRy', 'Knee_lrCorr_angle', 'Hip_IQR_vel_angle', 'Elbow_IQR_vel_angle', 'Wrist_IQRaccy', 'Wrist_IQRvely', 'Elbow_lrCorr_x']\n",
    "\n",
    "features = FEATURES  # Start with all 57 baseline features\n",
    "tot_k = len(features)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "doc-section-2",
   "metadata": {},
   "source": [
    "## 1.2 - Iterative Feature Selection Loop\n",
    "\n",
    "This section implements the core iterative feature selection algorithm:\n",
    "\n",
    "### Algorithm Overview\n",
    "1. **Initialize**: Start with all 57 features from `FEATURES` constant\n",
    "2. **Iterate**: For each trial (typically 8 trials to reach MIN_K=7):\n",
    "   - **Step A - Enhanced Adaptive RFE**: `bd.run_adaptive_rfe()`\n",
    "     - Performs 50 parallel RFE trials with noise injection\n",
    "     - Uses Random Forest (200 trees) with 3x5 repeated cross-validation\n",
    "     - Applies binomial test (α=0.05) to identify statistically significant features\n",
    "     - Retains ~90% of features that pass significance testing\n",
    "     - Returns reduced feature list (typically removes 10-20% per iteration)\n",
    "   \n",
    "   - **Step B - Bayesian Surprise Computation**: `bd.run_surprise_with_features()`\n",
    "     - Computes negative log-likelihood for each infant using selected features\n",
    "     - Calculates z-scores: `z = (NLL - μ_train) / σ_train`\n",
    "     - Converts to p-values: `p = 2 * SF(|z|)` where SF is survival function\n",
    "     - Stores surprise metrics for downstream ROC analysis\n",
    "   \n",
    "   - **Step C - ROC Metrics Evaluation**: `bd.compute_roc_metrics()`\n",
    "     - Computes sensitivity, specificity across p-value thresholds\n",
    "     - Calculates AUC (Area Under ROC Curve)\n",
    "     - Stores performance metrics indexed by feature count\n",
    "\n",
    "3. **Termination**: Loop continues until feature count ≤ `MIN_K` (7 features)\n",
    "\n",
    "### Method Name Note\n",
    "⚠️ **IMPORTANT**: The code uses legacy method names:\n",
    "- `run_adaptive_rfe()` → Current: `run_rfe_on_base()`\n",
    "- `compute_roc_metrics()` → Current: `run_metrics_from_surprise()`\n",
    "\n",
    "This notebook predates the API update and should be updated in future revisions.\n",
    "\n",
    "### Performance\n",
    "- **Per iteration**: 2-4 minutes (depends on feature count)\n",
    "- **Total runtime**: ~22 minutes for 8 iterations (57 → 54 → 48 → 45 → 40 → 34 → 24 → 15 → 7)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T22:45:53.266000Z",
     "start_time": "2025-10-05T22:23:36.990000Z"
    }
   },
   "source": [
    "tick = 1\n",
    "while True:\n",
    "    logger.debug(f\"Trial {tick}: Features in: {len(features)}...\")\n",
    "    \n",
    "    # Step A: Enhanced Adaptive RFE with statistical significance testing\n",
    "    logger.debug(f\"Running adaptive RFE...\")\n",
    "    features = bd.run_adaptive_rfe(\"real\", features, tot_k)\n",
    "    \n",
    "    # Step B: Compute Bayesian surprise using selected features\n",
    "    logger.debug(f\"Running surprise...\")\n",
    "    bd.run_surprise_with_features(\"real\", features, overwrite=True)\n",
    "    \n",
    "    # Step C: Evaluate ROC performance (sensitivity, specificity, AUC)\n",
    "    logger.debug(f\"Computing ROC...\")\n",
    "    metrics = bd.compute_roc_metrics(\"real\", len(features))\n",
    "    \n",
    "    logger.debug(f\"...Trial {tick}: Features out: {len(features)}.\")\n",
    "    tick += 1\n",
    "    \n",
    "    # Termination condition: stop when feature count reaches minimum threshold\n",
    "    if len(features) <= MIN_K:\n",
    "        break\n",
    "\n",
    "stop_time = datetime.now()\n",
    "logger.debug(f\"Completed Feature Selection in {(stop_time - start_time).seconds / 60: 0.2f} Minutes.\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "doc-section-3",
   "metadata": {},
   "source": [
    "## 1.3 - Results Export and Analysis\n",
    "\n",
    "This section exports comprehensive results and performs feature analysis:\n",
    "\n",
    "### Export Operations\n",
    "1. **Excel Report**: `bd.write_excel_report(\"real\")`\n",
    "   - Generates `/Volumes/secure/data/early_markers/cribsy/xlsx/real_*.xlsx`\n",
    "   - Contains multiple worksheets:\n",
    "     - Summary: Overall performance metrics across all iterations\n",
    "     - Detail: Per-iteration ROC curves and feature lists\n",
    "     - Features: Complete feature selection progression\n",
    "   - Formatted with conditional formatting for easy interpretation\n",
    "\n",
    "2. **Pickle Serialization**: `pickle.dump(bd, f)`\n",
    "   - Saves complete `BayesianData` object to `bd_real.pkl`\n",
    "   - Preserves all models, metrics, and intermediate results\n",
    "   - Used as input for subsequent analysis notebooks (e.g., `04_bam_sample_size.ipynb`)\n",
    "   - Can be loaded with: `with open(PKL_DIR / \"bd_real.pkl\", 'rb') as f: bd = pickle.load(f)`\n",
    "\n",
    "### Feature Analysis\n",
    "The console output provides three key summaries:\n",
    "\n",
    "1. **All Features in Models**: Total count across all iterations (with duplicates)\n",
    "2. **Deduped Features**: Unique features selected at any iteration\n",
    "   - Shows which features were consistently selected\n",
    "   - Typical result: ~54 unique features from 8 models\n",
    "3. **Common Features**: Intersection analysis\n",
    "   - Identifies features appearing in both base and selected sets\n",
    "   - Useful for validating feature stability\n",
    "\n",
    "### Interpreting Results\n",
    "- **High feature stability**: Features appearing in multiple iterations indicate robust predictive power\n",
    "- **Progressive reduction**: Feature count should decrease smoothly (57→54→48→45→40→34→24→15→7)\n",
    "- **Final feature set**: The 7-feature model represents the minimal effective feature set\n",
    "- **Body part distribution**: Expect features from multiple body parts (Ankle, Wrist, Knee, Hip, Shoulder)\n",
    "- **Metric types**: Balanced mix of position, velocity, acceleration, entropy, and correlation features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T22:45:53.339955Z",
     "start_time": "2025-10-05T22:45:53.266000Z"
    }
   },
   "source": [
    "# Export comprehensive Excel report with all metrics and summaries\n",
    "bd.write_excel_report(\"real\")\n",
    "\n",
    "# Serialize complete BayesianData object for downstream analysis\n",
    "with open(PKL_DIR / \"bd_real.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bd, f)\n",
    "\n",
    "# Feature Analysis: Extract all features used across all models\n",
    "l = [f for m in bd.metrics_names for f in bd.metrics(m).features ]\n",
    "print(f\"All Features in Models: {len(l)}\")\n",
    "\n",
    "# Deduplicate to find unique features selected at any iteration\n",
    "keeps = list(set(l))\n",
    "print(f\"\\nDeduped Features: {len(keeps)}\")\n",
    "keeps.sort()\n",
    "print(f\"\\nDeduped:\\n{keeps}\")\n",
    "\n",
    "# Compare with baseline features (excluding manually dropped features)\n",
    "print(f\"\\nBase Features not in Dropped:\\n{[f for f in bd.base_features if f not in drops]}\")\n",
    "\n",
    "# Identify features common to both selected and baseline sets\n",
    "common = [f for f in keeps if f in bd.base_features]\n",
    "common.extend([f for f in bd.base_features if f in keeps])\n",
    "common = sorted(list(set(common)))\n",
    "print(f\"\\ncommon features ({len(common)}):\\n{common}\")"
   ],
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
