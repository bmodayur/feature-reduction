{
 "cells": [
  {
   "cell_type": "code",
   "id": "45225509474bbedc",
   "metadata": {},
   "source": [
    "import pylab as pl\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "from early_markers.cribsy.common.constants import PKL_DIR, RAND_STATE\n",
    "from early_markers.cribsy.common.bayes import BayesianData\n",
    "\n",
    "\n",
    "np.random.seed(RAND_STATE)\n",
    "\n",
    "\n",
    "def beta_hdi(alpha, beta, ci=0.95):\n",
    "    \"\"\"Calculate HDI for Beta distribution using optimization\"\"\"\n",
    "    def interval_width(low):\n",
    "        high = low + ci\n",
    "        lower = stats.beta.ppf(low, alpha, beta)\n",
    "        upper = stats.beta.ppf(high, alpha, beta)\n",
    "        return abs(upper - lower)\n",
    "    \n",
    "    result = minimize_scalar(interval_width, bounds=(0, 1-ci), method='bounded')\n",
    "    low = result.x\n",
    "    high = low + ci\n",
    "    return stats.beta.ppf([low, high], alpha, beta)\n",
    "\n",
    "# ========================================================================\n",
    "# 1. Bayesian Assurance Method (BAM) for Model Development Sample Size\n",
    "# ========================================================================\n",
    "def revised_bam_model(pilot_data, hdi_width=0.15, ci=0.95, \n",
    "                     target_assurance=0.8, simulations=2000,\n",
    "                     max_sample=10000):\n",
    "    # Hyperpriors from pilot ESS\n",
    "    n_pilot = len(pilot_data)\n",
    "    p_pilot = np.mean(pilot_data)\n",
    "    pilot_ess = n_pilot\n",
    "    \n",
    "    alpha_shape = pilot_ess * p_pilot + 1\n",
    "    beta_shape = pilot_ess * (1 - p_pilot) + 1\n",
    "    \n",
    "    alpha_hyper = stats.gamma(alpha_shape, scale=1/pilot_ess)\n",
    "    beta_hyper = stats.gamma(beta_shape, scale=1/pilot_ess)\n",
    "    \n",
    "    # Binary search with stabilized estimates\n",
    "    low, high = max(50, int(n_pilot*0.5)), max_sample\n",
    "    best_n = max_sample\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        valid = 0\n",
    "        min_sims = 0\n",
    "        \n",
    "        while min_sims < simulations:\n",
    "            # Hierarchical sampling\n",
    "            a_h = alpha_hyper.rvs()\n",
    "            b_h = beta_hyper.rvs()\n",
    "            theta = stats.beta(a_h + np.sum(pilot_data), \n",
    "                              b_h + n_pilot - np.sum(pilot_data)).rvs()\n",
    "            \n",
    "            # Data generation\n",
    "            k = np.random.binomial(mid, theta)\n",
    "            a_post = a_h + np.sum(pilot_data) + k\n",
    "            b_post = b_h + n_pilot - np.sum(pilot_data) + mid - k\n",
    "            \n",
    "            # Accurate HDI calculation\n",
    "            lower, upper = beta_hdi(a_post, b_post, ci)\n",
    "            if (upper - lower) <= hdi_width:\n",
    "                valid += 1\n",
    "            min_sims += 1\n",
    "        \n",
    "        assurance = valid / simulations\n",
    "        if assurance >= target_assurance:\n",
    "            best_n = mid\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            low = mid + 1\n",
    "            \n",
    "    return best_n\n",
    "\n",
    "\n",
    "def ipw_informed_bam_performance(pilot_se, pilot_sp, prevalence_prior=(8,32),\n",
    "                             hdi_width=0.1, ci=0.95, target_assurance=0.8,\n",
    "                             simulations=1000, max_sample=5000,\n",
    "                             strata_props=None, strata_prev=None,\n",
    "                             strata_se_var=None, strata_sp_var=None,\n",
    "                             use_optimal_allocation=True):\n",
    "    \"\"\"\n",
    "    Sample size determination for diagnostic accuracy studies using inverse probability weighting.\n",
    "    \n",
    "    This function extends the informed_bam_performance function to incorporate stratified sampling\n",
    "    and inverse probability weighting, which can reduce the required sample size by optimizing\n",
    "    the allocation of samples across population strata.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pilot_se : tuple (TP, FN)\n",
    "        Pilot data for sensitivity (true positives, false negatives)\n",
    "    pilot_sp : tuple (TN, FP)\n",
    "        Pilot data for specificity (true negatives, false positives)\n",
    "    prevalence_prior : tuple (alpha, beta), default=(8, 32)\n",
    "        Beta prior parameters for disease prevalence\n",
    "    hdi_width : float, default=0.1\n",
    "        Target width for the highest density interval\n",
    "    ci : float, default=0.95\n",
    "        Confidence level for the highest density interval\n",
    "    target_assurance : float, default=0.8\n",
    "        Target probability of achieving the desired HDI width\n",
    "    simulations : int, default=1000\n",
    "        Number of simulations for the assurance calculation\n",
    "    max_sample : int, default=5000\n",
    "        Maximum sample size to consider\n",
    "    strata_props : list or array, default=None\n",
    "        Proportions of different strata in the target population\n",
    "        If None, assumes a single stratum (homogeneous population)\n",
    "    strata_prev : list or array, default=None\n",
    "        Expected prevalence in each stratum\n",
    "        If None, uses prevalence_prior for all strata\n",
    "    strata_se_var : list or array, default=None\n",
    "        Expected variance of sensitivity in each stratum\n",
    "        If None, estimates variance based on pilot data\n",
    "    strata_sp_var : list or array, default=None\n",
    "        Expected variance of specificity in each stratum\n",
    "        If None, estimates variance based on pilot data\n",
    "    use_optimal_allocation : bool, default=True\n",
    "        Whether to use optimal allocation (Neyman allocation) based on stratum variances\n",
    "        If False, uses proportional allocation based on stratum sizes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Minimum sample size required to achieve the target assurance\n",
    "    dict\n",
    "        Additional information including optimal allocation and effective sample size\n",
    "    \"\"\"\n",
    "    # Derive Beta priors from pilot data\n",
    "    se_alpha, se_beta = pilot_se[0] + 1, pilot_se[1] + 1\n",
    "    sp_alpha, sp_beta = pilot_sp[0] + 1, pilot_sp[1] + 1\n",
    "    \n",
    "    # Set up strata\n",
    "    if strata_props is None:\n",
    "        strata_props = [1.0]  # Single stratum\n",
    "    \n",
    "    n_strata = len(strata_props)\n",
    "    \n",
    "    if strata_prev is None:\n",
    "        # Use the same prevalence prior for all strata\n",
    "        prev_mean = prevalence_prior[0] / (prevalence_prior[0] + prevalence_prior[1])\n",
    "        strata_prev = [prev_mean] * n_strata\n",
    "    \n",
    "    # Estimate sensitivity and specificity variance if not provided\n",
    "    if strata_se_var is None:\n",
    "        se_mean = se_alpha / (se_alpha + se_beta)\n",
    "        se_var = (se_alpha * se_beta) / ((se_alpha + se_beta)**2 * (se_alpha + se_beta + 1))\n",
    "        strata_se_var = [se_var] * n_strata\n",
    "    \n",
    "    if strata_sp_var is None:\n",
    "        sp_mean = sp_alpha / (sp_alpha + sp_beta)\n",
    "        sp_var = (sp_alpha * sp_beta) / ((sp_alpha + sp_beta)**2 * (sp_alpha + sp_beta + 1))\n",
    "        strata_sp_var = [sp_var] * n_strata\n",
    "    \n",
    "    # Determine allocation of samples to strata\n",
    "    if use_optimal_allocation:\n",
    "        # For sensitivity, optimal allocation is proportional to stratum size * sqrt(variance)\n",
    "        # We consider both sensitivity and specificity by taking the maximum variance\n",
    "        se_alloc = [strata_props[i] * np.sqrt(strata_se_var[i]) for i in range(n_strata)]\n",
    "        sp_alloc = [strata_props[i] * np.sqrt(strata_sp_var[i]) for i in range(n_strata)]\n",
    "        \n",
    "        # Use the maximum of the two allocations for each stratum\n",
    "        opt_alloc = [max(se_alloc[i], sp_alloc[i]) for i in range(n_strata)]\n",
    "        sampling_props = [a / sum(opt_alloc) for a in opt_alloc]\n",
    "    else:\n",
    "        # Proportional allocation\n",
    "        sampling_props = strata_props\n",
    "    \n",
    "    # Calculate inverse probability weights\n",
    "    ipw = [strata_props[i] / sampling_props[i] for i in range(n_strata)]\n",
    "    \n",
    "    # Calculate design effect\n",
    "    deff = sum([(ipw[i]**2) * sampling_props[i] for i in range(n_strata)])\n",
    "    \n",
    "    # Calculate effective sample size reduction factor\n",
    "    ess_factor = 1 / deff\n",
    "    \n",
    "    # Binary search setup with realistic bounds\n",
    "    low = max(100, int(100 * ess_factor))  # Adjust lower bound based on effective sample size\n",
    "    high = max_sample\n",
    "    best_n = max_sample\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        valid = 0\n",
    "        min_cases = max(10, int(0.05*mid))  # At least 5% cases per class\n",
    "        \n",
    "        for _ in range(simulations):\n",
    "            # Allocate samples to strata based on sampling proportions\n",
    "            stratum_sizes = np.random.multinomial(mid, sampling_props)\n",
    "            \n",
    "            # Generate data for each stratum\n",
    "            weighted_tp = 0\n",
    "            weighted_fn = 0\n",
    "            weighted_tn = 0\n",
    "            weighted_fp = 0\n",
    "            \n",
    "            total_pos = 0\n",
    "            total_neg = 0\n",
    "            \n",
    "            for i in range(n_strata):\n",
    "                # Skip empty strata\n",
    "                if stratum_sizes[i] == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Sample prevalence for this stratum\n",
    "                prev = stats.beta(*prevalence_prior).rvs() if len(strata_prev) == 1 else strata_prev[i]\n",
    "                \n",
    "                # Sample sensitivity and specificity\n",
    "                se = stats.beta(se_alpha, se_beta).rvs()\n",
    "                sp = stats.beta(sp_alpha, sp_beta).rvs()\n",
    "                \n",
    "                # Generate synthetic data with prevalence floor\n",
    "                n_pos = max(1, np.random.binomial(stratum_sizes[i], prev)) if stratum_sizes[i] > 0 else 0\n",
    "                n_neg = stratum_sizes[i] - n_pos\n",
    "                \n",
    "                total_pos += n_pos\n",
    "                total_neg += n_neg\n",
    "                \n",
    "                if n_pos > 0:\n",
    "                    tp = np.random.binomial(n_pos, se)\n",
    "                    fn = n_pos - tp\n",
    "                else:\n",
    "                    tp, fn = 0, 0\n",
    "                \n",
    "                if n_neg > 0:\n",
    "                    tn = np.random.binomial(n_neg, sp)\n",
    "                    fp = n_neg - tn\n",
    "                else:\n",
    "                    tn, fp = 0, 0\n",
    "                \n",
    "                # Apply inverse probability weight\n",
    "                w = ipw[i]\n",
    "                weighted_tp += tp * w\n",
    "                weighted_fn += fn * w\n",
    "                weighted_tn += tn * w\n",
    "                weighted_fp += fp * w\n",
    "            \n",
    "            # Ensure minimum cases per class\n",
    "            if total_pos < min_cases or total_neg < min_cases:\n",
    "                continue\n",
    "            \n",
    "            # Calculate posteriors with weighted counts\n",
    "            se_post_alpha = se_alpha + weighted_tp\n",
    "            se_post_beta = se_beta + weighted_fn\n",
    "            sp_post_alpha = sp_alpha + weighted_tn\n",
    "            sp_post_beta = sp_beta + weighted_fp\n",
    "            \n",
    "            # Calculate HDI widths\n",
    "            se_hdi = beta_hdi(se_post_alpha, se_post_beta, ci)\n",
    "            sp_hdi = beta_hdi(sp_post_alpha, sp_post_beta, ci)\n",
    "            \n",
    "            if (se_hdi[1] - se_hdi[0] <= hdi_width and\n",
    "                sp_hdi[1] - sp_hdi[0] <= hdi_width):\n",
    "                valid += 1\n",
    "        \n",
    "        assurance = valid / simulations\n",
    "        \n",
    "        if assurance >= target_assurance:\n",
    "            best_n = mid\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            low = mid + 1\n",
    "    \n",
    "    # Return additional information about the design\n",
    "    info = {\n",
    "        'sampling_props': sampling_props,\n",
    "        'ipw': ipw,\n",
    "        'design_effect': deff,\n",
    "        'ess_factor': ess_factor,\n",
    "        'estimated_reduction': 1 - (best_n / (best_n / ess_factor))\n",
    "    }\n",
    "    \n",
    "    return best_n, info\n",
    "\n",
    "\n",
    "# def informed_bam_performance(pilot_se, pilot_sp, prevalence_prior=(8,32),\n",
    "#                             hdi_width=0.1, ci=0.95, target_assurance=0.8,\n",
    "#                             simulations=1000, max_sample=5000):\n",
    "#     \"\"\"\n",
    "#     Optimized BAM implementation for performance evaluation\n",
    "#     \n",
    "#     Parameters:\n",
    "#     pilot_se (tuple): (TP, FN) from pilot data\n",
    "#     pilot_sp (tuple): (TN, FP) from pilot data\n",
    "#     prevalence_prior (tuple): Beta parameters for prevalence\n",
    "#     hdi_width (float): Desired HDI width\n",
    "#     ci (float): Credible interval level\n",
    "#     target_assurance (float): Required joint assurance probability\n",
    "#     simulations (int): Number of simulations per sample size\n",
    "#     max_sample (int): Maximum allowable sample size\n",
    "#     \n",
    "#     Returns:\n",
    "#     Optimal sample size (int)\n",
    "#     \"\"\"\n",
    "#     # Derive Beta priors from pilot data\n",
    "#     se_alpha, se_beta = pilot_se[0] + 1, pilot_se[1] + 1\n",
    "#     sp_alpha, sp_beta = pilot_sp[0] + 1, pilot_sp[1] + 1\n",
    "#     \n",
    "#     # Binary search setup with realistic bounds\n",
    "#     low, high = 100, max_sample\n",
    "#     best_n = max_sample\n",
    "#     \n",
    "#     while low <= high:\n",
    "#         mid = (low + high) // 2\n",
    "#         valid = 0\n",
    "#         min_cases = max(10, int(0.05*mid))  # At least 5% cases per class\n",
    "#         \n",
    "#         for _ in range(simulations):\n",
    "#             # Sample parameters from informed priors\n",
    "#             se = stats.beta(se_alpha, se_beta).rvs()\n",
    "#             sp = stats.beta(sp_alpha, sp_beta).rvs()\n",
    "#             prev = stats.beta(*prevalence_prior).rvs()\n",
    "#             \n",
    "#             # Generate synthetic data with prevalence floor\n",
    "#             n_pos = max(min_cases, np.random.binomial(mid, prev))\n",
    "#             n_neg = max(min_cases, mid - n_pos)\n",
    "#             \n",
    "#             tp = np.random.binomial(n_pos, se)\n",
    "#             tn = np.random.binomial(n_neg, sp)\n",
    "#             \n",
    "#             # Calculate posteriors\n",
    "#             se_post_alpha = se_alpha + tp\n",
    "#             se_post_beta = se_beta + (n_pos - tp)\n",
    "#             sp_post_alpha = sp_alpha + tn\n",
    "#             sp_post_beta = sp_beta + (n_neg - tn)\n",
    "#             \n",
    "#             # Calculate HDI widths\n",
    "#             se_hdi = beta_hdi(se_post_alpha, se_post_beta, ci)\n",
    "#             sp_hdi = beta_hdi(sp_post_alpha, sp_post_beta, ci)\n",
    "#             \n",
    "#             if (se_hdi[1] - se_hdi[0] <= hdi_width and \n",
    "#                 sp_hdi[1] - sp_hdi[0] <= hdi_width):\n",
    "#                 valid += 1\n",
    "#                 \n",
    "#         assurance = valid / simulations\n",
    "#         \n",
    "#         if assurance >= target_assurance:\n",
    "#             best_n = mid\n",
    "#             high = mid - 1\n",
    "#         else:\n",
    "#             low = mid + 1\n",
    "#             \n",
    "#     return best_n\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "import polars as pl\n",
    "from polars import DataFrame\n",
    "\n",
    "from early_markers.cribsy.common.constants import PKL_DIR, RAND_STATE\n",
    "from early_markers.cribsy.common.bayes import BayesianData\n",
    "\n",
    "\n",
    "with open(PKL_DIR / \"bd_real.pkl\", \"rb\") as f:\n",
    "    bd: BayesianData = pickle.load(f)\n",
    "    \n",
    "metrics = bd.metrics(\"real_k_19\")\n",
    "\n",
    "# perf = (\n",
    "#     metrics.metrics\n",
    "#     .filter(pl.col(\"thresh\").round(5) == round(metrics.threshold_j, 5))\n",
    "#     .to_dicts()[0]\n",
    "# )\n",
    "\n",
    "prims = (\n",
    "    metrics.primitives\n",
    "    .filter(pl.col(\"thresh\").round(5) == round(metrics.threshold_j, 5))\n",
    "    .with_columns(\n",
    "        hits=pl.col(\"tp\") + pl.col(\"tn\")\n",
    "    ).to_dicts()[0]\n",
    ")\n",
    "total = metrics.test_n\n",
    "\n",
    "pos = [1 for _ in range(prims[\"hits\"])]\n",
    "neg = [0 for _ in range(total - prims[\"hits\"])]\n",
    "pilot_data = pos + neg\n",
    "\n",
    "dev_sample = revised_bam_model(pilot_data, hdi_width=0.2, max_sample=1000)\n",
    "print(f\"BAM Model Development Sample Size: {dev_sample}\")\n",
    "\n",
    "# Artificially inflate variance estimate for positive stratum\n",
    "strata_se_var = [0.25, 0.1]  # High variance for positives, low for negatives\n",
    "strata_sp_var = [0.1, 0.25]  # Reverse pattern for specificity\n",
    "\n",
    "# sens 0.8265714285714286 = TP / (TP + FN) = \n",
    "# spec 0.6857142857142857\n",
    "\n",
    "# Pilot data: 80 TP, 20 FN (sens 0.8), 90 TN, 10 FP (spec 0.9)\n",
    "perf_sample, design_info = ipw_informed_bam_performance(\n",
    "    pilot_se=(prims[\"tp\"], prims[\"fn\"]),\n",
    "    pilot_sp=(prims[\"tn\"], prims[\"fp\"]),\n",
    "    prevalence_prior=(4, 32),  # Beta(8,32) ≈ mean 0.2\n",
    "    hdi_width=0.2,\n",
    "    strata_props=[0.10, 0.90],\n",
    "    strata_prev=[0.90, 0.05],\n",
    "    strata_se_var=strata_se_var,\n",
    "    strata_sp_var=strata_sp_var,\n",
    "    simulations=2000,\n",
    ")\n",
    "\n",
    "# sample_size, design_info = ipw_informed_bam_performance(\n",
    "#     pilot_se=(45,5), \n",
    "#     pilot_sp=(90,10),\n",
    "#     strata_props=[0.1, 0.9],\n",
    "#     sampling_props=[0.5, 0.5],\n",
    "#     use_optimal_allocation=False\n",
    "# )\n",
    "\n",
    "print(f\"BAM Performance Evaluation Sample Size: {perf_sample}\")\n",
    "\n",
    "# CI Width: 0.25\n",
    "# BAM Model Development Sample Size: 50\n",
    "# BAM Performance Evaluation Sample Size: 372"
   ],
   "id": "cc3d5e098d768d67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stats.beta(4, 28).mean()",
   "id": "ac9411a05cee515c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Generate pilot data (60% positive outcomes)\n",
    "\n",
    "pilot_data = np.random.binomial(1, 0.6, 50)\n",
    "\n",
    "# 1. Model development sample size\n",
    "dev_sample = bam_model_development(pilot_data, hdi_width=0.15)\n",
    "print(f\"BAM Model Development Sample Size: {dev_sample}\")\n",
    "\n",
    "# 2. Performance evaluation sample size\n",
    "# Using Gamma(10,2) priors for Se/Sp Beta parameters\n",
    "# Beta(1,1) prior for prevalence (uniform)\n",
    "perf_sample = bam_performance_evaluation(prior_se=(10,2), prior_sp=(10,2))\n",
    "print(f\"BAM Performance Evaluation Sample Size: {perf_sample}\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Pilot data: 80 TP, 20 FN (sens 0.8), 90 TN, 10 FP (spec 0.9)\n",
    "    sample_size = informed_bam_performance(\n",
    "        pilot_se=(80, 20),\n",
    "        pilot_sp=(90, 10),\n",
    "        prevalence_prior=(8, 32),  # Beta(8,32) ≈ mean 0.2\n",
    "        hdi_width=0.2,\n",
    "        simulations=2000,\n",
    "    )\n",
    "    print(f\"Optimized sample size: {sample_size}\")\n"
   ],
   "id": "cb79a2bdcac1d917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def bam_performance_evaluation(prior_se=(10, 2), prior_sp=(10, 2), prior_prev=(1, 1),\n",
    "                              hdi_width=0.1, ci=0.95, target_assurance=0.8,\n",
    "                              simulations=500, max_sample=10000):\n",
    "    \"\"\"\n",
    "    BAM implementation for classification performance evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    prior_se (tuple): Gamma hyperparameters for sensitivity Beta prior\n",
    "    prior_sp (tuple): Gamma hyperparameters for specificity Beta prior\n",
    "    prior_prev (tuple): Beta parameters for prevalence\n",
    "    hdi_width (float): Desired HDI width for both Se and Sp\n",
    "    ci (float): Credible interval level\n",
    "    target_assurance (float): Required joint assurance probability\n",
    "    simulations (int): Number of prior samples per evaluation\n",
    "    max_sample (int): Maximum allowable sample size\n",
    "    \n",
    "    Returns:\n",
    "    Optimal sample size (int)\n",
    "    \"\"\"\n",
    "    # Binary search setup\n",
    "    low, high = 10, max_sample\n",
    "    best_n = max_sample\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        joint_assurance = 0\n",
    "        \n",
    "        for _ in range(simulations):\n",
    "            # Sample hyperparameters\n",
    "            a_se = stats.gamma(*prior_se).rvs()\n",
    "            b_se = stats.gamma(*prior_se).rvs()\n",
    "            a_sp = stats.gamma(*prior_sp).rvs()\n",
    "            b_sp = stats.gamma(*prior_sp).rvs()\n",
    "            \n",
    "            # Sample true parameters\n",
    "            se = stats.beta(a_se, b_se).rvs()\n",
    "            sp = stats.beta(a_sp, b_sp).rvs()\n",
    "            prev = stats.beta(*prior_prev).rvs()\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            n_pos = np.random.binomial(mid, prev)\n",
    "            n_neg = mid - n_pos\n",
    "            \n",
    "            tp = np.random.binomial(n_pos, se)\n",
    "            fn = n_pos - tp\n",
    "            tn = np.random.binomial(n_neg, sp)\n",
    "            fp = n_neg - tn\n",
    "            \n",
    "            # Update posteriors\n",
    "            a_se_post = a_se + tp\n",
    "            b_se_post = b_se + fn\n",
    "            a_sp_post = a_sp + tn\n",
    "            b_sp_post = b_sp + fp\n",
    "            \n",
    "            # Check HDI widths\n",
    "            se_lower, se_upper = beta_hdi(a_se_post, b_se_post, ci)\n",
    "            sp_lower, sp_upper = beta_hdi(a_sp_post, b_sp_post, ci)\n",
    "            \n",
    "            if (se_upper - se_lower <= hdi_width) and (sp_upper - sp_lower <= hdi_width):\n",
    "                joint_assurance += 1\n",
    "                \n",
    "        assurance_prob = joint_assurance / simulations\n",
    "        \n",
    "        if assurance_prob >= target_assurance:\n",
    "            best_n = mid\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            low = mid + 1\n",
    "            \n",
    "    return best_n"
   ],
   "id": "53f0396b25e8eb95",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
