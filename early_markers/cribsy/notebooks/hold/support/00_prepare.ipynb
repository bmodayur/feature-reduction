{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "doc-main-summary",
   "metadata": {},
   "source": [
    "# 00 - Data Preparation\n",
    "\n",
    "**Purpose**: This notebook serves as the foundational step in the `early-markers` analysis pipeline. It is responsible for loading the raw data, performing essential cleaning and transformations, and structuring the dataset for all subsequent exploratory data analysis (EDA), feature selection, and modeling tasks.\n",
    "\n",
    "**Inputs**:\n",
    "- `features_merged.pkl`: The raw feature data stored as a Pandas DataFrame pickle file, located in `early_markers/emmacp_metrics/`.\n",
    "\n",
    "**Outputs**:\n",
    "- An in-memory, transformed Polars DataFrame. This notebook's primary output is the cleaned data used to instantiate the `BayesianData` class in other notebooks, rather than saving a new file.\n",
    "\n",
    "### Key Transformation Steps:\n",
    "1.  **Load Raw Data**: Reads the initial pickle file.\n",
    "2.  **Filter Invalid Records**: Removes known bad data points.\n",
    "3.  **Encode Risk**: Converts the multi-level `risk_raw` score into a binary `risk` classification (0: Normal, 1: At-Risk).\n",
    "4.  **Assign Category**: Segregates data into `category` 1 (Training) and 2 (Testing) based on risk and original category assignments.\n",
    "5.  **Unify Feature Column**: Creates a single, descriptive `feature` name from `part` and `feature_name`.\n",
    "6.  **Integrate Age**: Reshapes the data to include `age_in_weeks` as a feature for each infant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc-cell-1",
   "metadata": {},
   "source": [
    "### 1.1 Imports and Initial Correlation Analysis\n",
    "\n",
    "This cell performs the following actions:\n",
    "- **Imports Libraries**: Imports necessary libraries such as `polars` for data manipulation and modules from the local `early_markers` codebase.\n",
    "- **Instantiates `BayesianData`**: Creates an instance of the main data handling class, which loads and preprocesses the data in its constructor.\n",
    "- **Computes Correlation Matrix**: Calculates the feature correlation matrix on the wide-format DataFrame to identify highly correlated features.\n",
    "- **Identifies Features to Drop**: Establishes two criteria for feature removal:\n",
    "  1. `hi_corrs`: Features with a pairwise correlation greater than `CORR_MAX` (0.9).\n",
    "  2. `lo_risk_corrs`: Features with a correlation to the `risk` target variable less than `CORR_MIN` (0.1).\n",
    "- **Consolidates Drops**: Combines these lists to create a final set of `drops` to be excluded from modeling."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import polars as pl\n",
    "from polars import DataFrame\n",
    "import polars.selectors as cs\n",
    "\n",
    "from early_markers.cribsy.common.constants import CSV_DIR, FEATURES\n",
    "from early_markers.cribsy.common.bayes import BayesianData\n",
    "\n",
    "CORR_MIN = 0.1\n",
    "CORR_MAX = 0.9\n",
    "\n",
    "bd = BayesianData()\n",
    "df_base = bd.base_wide.drop(\"Shoulder_lrCorr_x\")\n",
    "df = df_base.select([\"risk\"] + FEATURES).corr().with_columns(\n",
    "    feature=pl.Series([\"risk\"] + FEATURES)\n",
    ")\n",
    "df_corr = df.select([df.columns[-1]] + df.columns[:-1])\n",
    "\n",
    "hi_corrs = {}\n",
    "for row in df_corr.rows(named=True):\n",
    "    for k, v in row.items():\n",
    "        if k == \"feature\":\n",
    "            continue\n",
    "        if k == row[\"feature\"]:\n",
    "            continue\n",
    "        if k in hi_corrs:\n",
    "            continue\n",
    "        if abs(v) >= CORR_MAX:\n",
    "            if row[\"feature\"] not in hi_corrs:\n",
    "                hi_corrs[row[\"feature\"]] = []\n",
    "            hi_corrs[row[\"feature\"]].append(\n",
    "                {\n",
    "                    \"correlate\": k,\n",
    "                    \"value\": v,\n",
    "                }\n",
    "            )\n",
    "\n",
    "lo_risk_corrs = {}\n",
    "rows = df_corr.rows(named=True)\n",
    "risk = rows[0]\n",
    "\n",
    "for k, v in risk.items():\n",
    "    if k in [\"feature\", \"risk\"]:\n",
    "        continue\n",
    "    if abs(v) < CORR_MIN:\n",
    "        lo_risk_corrs[k] = v\n",
    "\n",
    "losers = []\n",
    "for k, v in hi_corrs.items():\n",
    "    l = []\n",
    "    for v2 in v:\n",
    "        if abs(risk[k]) > v2[\"value\"]:\n",
    "            l.append(v2[\"correlate\"])\n",
    "    if len(l) > 0:\n",
    "        losers.extend(l)\n",
    "    else:\n",
    "        losers.append(k)\n",
    "\n",
    "losers.extend(lo_risk_corrs.keys())\n",
    "drops = list(set(losers))\n",
    "\n",
    "# drops = ['Shoulder_IQR_vel_angle', 'Ankle_IQRaccx', 'Wrist_IQRaccx', 'Ankle_IQRvelx', 'Knee_IQR_vel_angle', 'Elbow_IQR_acc_angle', 'Shoulder_mean_angle', 'Ankle_IQRaccy', 'Shoulder_lrCorr_angle', 'Hip_entropy_angle', 'Elbow_mean_angle', 'Eye_lrCorr_x', 'Shoulder_entropy_angle', 'Knee_entropy_angle', 'Shoulder_IQR_acc_angle', 'Ankle_lrCorr_x', 'Hip_lrCorr_angle', 'Wrist_meanent', 'Wrist_IQRvelx', 'Wrist_mediany', 'Ankle_IQRvely', 'Shoulder_stdev_angle', 'Hip_IQR_acc_angle', 'Elbow_stdev_angle', 'Knee_IQR_acc_angle', 'Ankle_meanent', 'Ankle_medianx', 'Wrist_IQRy', 'Knee_lrCorr_angle', 'Hip_IQR_vel_angle', 'Elbow_IQR_vel_angle', 'Wrist_IQRaccy', 'Wrist_IQRvely', 'Elbow_lrCorr_x']"
   ],
   "id": "6cd65b03ecee1e3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "doc-cell-2",
   "metadata": {},
   "source": [
    "### 1.2 Data Loading and Transformation\n",
    "\n",
    "This cell executes the core data preparation logic. It reads the raw data from the pickle file and applies a series of `polars` transformations in a single, efficient chain:\n",
    "1.  **Load Data**: Reads the `features_merged.pkl` file into a Pandas DataFrame and immediately converts it to a Polars DataFrame for high-performance processing.\n",
    "2.  **Filter**: Removes known invalid data points (`part == 'umber'` and `infant == 'clin_100_6'`).\n",
    "3.  **Rename**: Renames columns for clarity (e.g., `Value` to `value`).\n",
    "4.  **Encode Risk**: Creates the binary `risk` column based on the `risk_raw` value (`<= 1` is 0, `> 1` is 1).\n",
    "5.  **Assign Category**: Creates the `category` column (1 for training, 2 for testing) based on the newly created `risk` column and the original `category`.\n",
    "6.  **Create Feature Name**: Concatenates `part` and `feature_name` to create a single, unique identifier for each feature.\n",
    "7.  **Drop Columns**: Removes original columns that are now redundant (`part`, `feature_name`, `age_bracket`)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from early_markers.cribsy.common.constants import RAW_DATA\n",
    "\n",
    "\n",
    "pd_raw = pd.read_pickle(RAW_DATA)\n",
    "df = (\n",
    "    pl.from_pandas(pd_raw)\n",
    "    .filter(pl.col(\"part\") != \"umber\", pl.col(\"infant\") != \"clin_100_6\")\n",
    "    .rename({\"risk\": \"risk_raw\", \"Value\": \"value\"})\n",
    "    .with_columns(\n",
    "        risk=pl.when(pl.col(\"risk_raw\") <= 1).then(0)\n",
    "        .otherwise(1),\n",
    "    ).with_columns(\n",
    "        category=pl.when((pl.col(\"category\") == 0) | (pl.col(\"risk\") == 0)).then(1)\n",
    "        .otherwise(2),\n",
    "        feature=pl.concat_str(pl.col(\"part\"), pl.col(\"feature_name\"), separator=\"_\")\n",
    "    ).drop([\"part\", \"feature_name\", \"age_bracket\"])\n",
    ")"
   ],
   "id": "98af1a81423cc5c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Reshape Age as a Feature\n",
    "\n",
    "This cell reshapes the data to treat `age_in_weeks` as its own feature, consistent with all other movement features.\n",
    "1.  **Extract Unique Age**: Creates a temporary DataFrame (`df2`) containing the unique `age_in_weeks` for each infant.\n",
    "2.  **Create Age Feature**: Adds a new `feature` column with the literal value `\"age_in_weeks\"` and copies the age value into the `value` column.\n",
    "3.  **Stack DataFrames**: Vertically stacks this new age-based DataFrame with the original feature DataFrame.\n",
    "4.  **Sort**: Sorts the final, combined DataFrame by infant and feature name, ensuring a clean and orderly dataset for subsequent analysis. The resulting shape is displayed."
   ],
   "id": "aa601c879398e209"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df2 = (\n",
    "    df.select([\"infant\", \"category\", \"risk\", \"age_in_weeks\"])\n",
    "    .unique()\n",
    "    .with_columns(\n",
    "        feature=pl.lit(\"age_in_weeks\"),\n",
    "        value=pl.col(\"age_in_weeks\")\n",
    "    )\n",
    ").drop(\"age_in_weeks\")\n",
    "\n",
    "df.select([\"infant\", \"category\", \"risk\", \"feature\", \"value\"]).vstack(df2).sort([\"infant\", \"feature\"])\n"
   ],
   "id": "9fc1e040b8cbb8b4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
