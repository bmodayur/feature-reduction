{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "doc-main-summary",
   "metadata": {},
   "source": [
    "# 09 - BAM for Bayesian Surprise\n",
    "\n",
    "**Purpose**: This notebook implements a Bayesian Assurance Model (BAM) simulation for planning new studies to determine the sample size needed to achieve a target level of Bayesian surprise, as measured by the Kullbackâ€“Leibler (KL) divergence between a prior and posterior distribution.\n",
    "\n",
    "**Inputs**: None (this notebook uses simulated data based on theoretical parameters).\n",
    "\n",
    "**Outputs**:\n",
    "- A plot showing the relationship between sample size and both the mean Bayesian surprise and the probability of achieving the target surprise level.\n",
    "- Console output indicating the minimum sample size required to achieve 80% power (assurance) for the specified surprise threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc-cell-1",
   "metadata": {},
   "source": [
    "### 9.1 BAM Simulation for Bayesian Surprise\n",
    "\n",
    "This cell defines the functions and runs the complete simulation...\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import kl_div\n",
    "\n",
    "def calculate_bayesian_surprise(prior_samples, posterior_samples):\n",
    "    \"\"\"Calculate Bayesian surprise using KL divergence\"\"\"\n",
    "    # Estimate densities for prior and posterior\n",
    "    prior_density, bins = np.histogram(prior_samples, bins=30, density=True)\n",
    "    posterior_density, _ = np.histogram(posterior_samples, bins=bins, density=True)\n",
    "\n",
    "    # Avoid division by zero in KL divergence\n",
    "    prior_density = np.clip(prior_density, 1e-10, None)\n",
    "    posterior_density = np.clip(posterior_density, 1e-10, None)\n",
    "\n",
    "    # Calculate KL divergence\n",
    "    surprise = np.sum(kl_div(posterior_density, prior_density))\n",
    "    return surprise\n",
    "\n",
    "def simulate_sample_size_for_surprise(min_sample_size=10, max_sample_size=100,\n",
    "                                     step_size=10, min_surprise=0.5,\n",
    "                                     true_param=0.65, prior_alpha=1, prior_beta=1,\n",
    "                                     n_sims=100):\n",
    "    \"\"\"\n",
    "    Determine sample size needed to achieve a target Bayesian surprise level\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_sample_size, max_sample_size, step_size: range of sample sizes to test\n",
    "    min_surprise: minimum Bayesian surprise threshold\n",
    "    true_param: true parameter value for simulation\n",
    "    prior_alpha, prior_beta: parameters of Beta prior\n",
    "    n_sims: number of simulations per sample size\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary with sample sizes and corresponding surprise values\n",
    "    \"\"\"\n",
    "    sample_sizes = range(min_sample_size, max_sample_size + 1, step_size)\n",
    "    results = {'sample_size': [], 'mean_surprise': [], 'std_surprise': [],\n",
    "               'prop_above_threshold': []}\n",
    "\n",
    "    # Generate prior samples once (same for all sample sizes)\n",
    "    prior_samples = np.random.beta(prior_alpha, prior_beta, size=10000)\n",
    "\n",
    "    for n in sample_sizes:\n",
    "        surprise_values = []\n",
    "\n",
    "        for _ in range(n_sims):\n",
    "            # Generate simulated data based on true parameter\n",
    "            data = np.random.binomial(1, true_param, size=n)\n",
    "\n",
    "            # Build and sample from the posterior\n",
    "            with pm.Model() as model:\n",
    "                theta = pm.Beta('theta', alpha=prior_alpha, beta=prior_beta)\n",
    "                obs = pm.Bernoulli('obs', p=theta, observed=data)\n",
    "                posterior = pm.sample(1000, tune=1000, chains=2, progressbar=False)\n",
    "\n",
    "            # Extract posterior samples\n",
    "            posterior_samples = posterior.posterior['theta'].values.flatten()\n",
    "\n",
    "            # Calculate Bayesian surprise\n",
    "            surprise = calculate_bayesian_surprise(prior_samples, posterior_samples)\n",
    "            surprise_values.append(surprise)\n",
    "\n",
    "        # Store results\n",
    "        results['sample_size'].append(n)\n",
    "        results['mean_surprise'].append(np.mean(surprise_values))\n",
    "        results['std_surprise'].append(np.std(surprise_values))\n",
    "        results['prop_above_threshold'].append(\n",
    "            np.mean([s >= min_surprise for s in surprise_values]))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the simulation\n",
    "np.random.seed(42)\n",
    "results = simulate_sample_size_for_surprise(min_sample_size=10, max_sample_size=100,\n",
    "                                           step_size=10, min_surprise=0.5)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.errorbar(results['sample_size'], results['mean_surprise'],\n",
    "             yerr=results['std_surprise'], marker='o')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Target surprise threshold')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Mean Bayesian Surprise (KL divergence)')\n",
    "plt.title('Mean Bayesian Surprise vs Sample Size')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(results['sample_size'], results['prop_above_threshold'], marker='o')\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', label='80% power threshold')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Proportion of Simulations Above Threshold')\n",
    "plt.title('Probability of Achieving Target Surprise vs Sample Size')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the minimum sample size that achieves 80% power\n",
    "min_required_n = next((n for n, prop in zip(results['sample_size'],\n",
    "                                          results['prop_above_threshold'])\n",
    "                      if prop >= 0.8), None)\n",
    "\n",
    "print(f\"Minimum sample size required: {min_required_n}\")\n"
   ],
   "id": "ff5460435827c18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.special import psi, gammaln\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def beta_kl_divergence(alpha1, beta1, alpha2, beta2):\n",
    "    \"\"\"Calculate KL divergence between two Beta distributions using vectorized operations\"\"\"\n",
    "    return (\n",
    "        gammaln(alpha1 + beta1) - gammaln(alpha2 + beta2)\n",
    "        + gammaln(alpha2) + gammaln(beta2) - gammaln(alpha1) - gammaln(beta1)\n",
    "        + (alpha1 - alpha2) * psi(alpha1)\n",
    "        + (beta1 - beta2) * psi(beta1)\n",
    "        + (alpha2 - alpha1 + beta2 - beta1) * psi(alpha1 + beta1)\n",
    "    )\n",
    "\n",
    "\n",
    "def diagnostic_sample_size_simulation(prior_alpha, prior_beta,\n",
    "                                      disease_prev, normal_prev,\n",
    "                                      threshold=0.2,\n",
    "                                      min_n=10, max_n=200, step=10,\n",
    "                                      n_sims=500, power=0.8):\n",
    "    \"\"\"\n",
    "    Bayesian sample size estimation for diagnostic surprise model\n",
    "\n",
    "    Parameters:\n",
    "        prior_alpha, prior_beta: Beta prior parameters for normal class\n",
    "        disease_prev: True prevalence in disease population (0-1)\n",
    "        normal_prev: True prevalence in normal population (0-1)\n",
    "        threshold: Surprise cutoff for classification\n",
    "        min_n, max_n, step: Sample size range to test\n",
    "        n_sims: Number of simulations per sample size\n",
    "        power: Required sensitivity/specificity probability\n",
    "\n",
    "    Returns:\n",
    "        Sample size results and optimal N\n",
    "    \"\"\"\n",
    "\n",
    "    sample_sizes = range(min_n, max_n+1, step)\n",
    "    results = {\n",
    "        'n': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': []\n",
    "    }\n",
    "\n",
    "    for n in sample_sizes:\n",
    "        disease_surprise = []\n",
    "        normal_surprise = []\n",
    "\n",
    "        # Simulate disease cases\n",
    "        for _ in range(n_sims):\n",
    "            data = np.random.binomial(1, disease_prev, n)\n",
    "            successes = data.sum()\n",
    "            post_alpha = prior_alpha + successes\n",
    "            post_beta = prior_beta + (n - successes)\n",
    "            surprise = beta_kl_divergence(post_alpha, post_beta,\n",
    "                                        prior_alpha, prior_beta)\n",
    "            disease_surprise.append(surprise)\n",
    "\n",
    "        # Simulate normal cases\n",
    "        for _ in range(n_sims):\n",
    "            data = np.random.binomial(1, normal_prev, n)\n",
    "            successes = data.sum()\n",
    "            post_alpha = prior_alpha + successes\n",
    "            post_beta = prior_beta + (n - successes)\n",
    "            surprise = beta_kl_divergence(post_alpha, post_beta,\n",
    "                                        prior_alpha, prior_beta)\n",
    "            normal_surprise.append(surprise)\n",
    "\n",
    "        # Calculate classification metrics\n",
    "        sens = np.mean(np.array(disease_surprise) <= threshold)\n",
    "        spec = np.mean(np.array(normal_surprise) > threshold)\n",
    "\n",
    "        results['n'].append(n)\n",
    "        results['sensitivity'].append(sens)\n",
    "        results['specificity'].append(spec)\n",
    "\n",
    "    # Find optimal sample size\n",
    "    # In diagnostic_sample_size_simulation()\n",
    "    optimal_n = None\n",
    "    for i, (n, sens, spec) in enumerate(zip(results['n'],\n",
    "                                          results['sensitivity'],\n",
    "                                          results['specificity'])):\n",
    "        if sens >= power and spec >= power:\n",
    "            optimal_n = n\n",
    "            break  # Return first n meeting criteria\n",
    "\n",
    "    # If no n meets criteria,\n",
    "    # Find closest sample size meeting criteria\n",
    "    if optimal_n is None:\n",
    "        # Use indices instead of sample size values\n",
    "        indices = range(len(results['n']))\n",
    "\n",
    "        closest_idx = min(indices,\n",
    "                         key=lambda i: (\n",
    "                             abs(results['sensitivity'][i] - power) +\n",
    "                             abs(results['specificity'][i] - power)\n",
    "                         ))\n",
    "\n",
    "        closest_n = results['n'][closest_idx]\n",
    "        print(f\"No sample size achieved {power*100}% power. Closest: {closest_n}\")\n",
    "        print(f\"Sensitivity: {results['sensitivity'][closest_idx]:.2f}\")\n",
    "        print(f\"Specificity: {results['specificity'][closest_idx]:.2f}\")\n",
    "\n",
    "\n",
    "    # Plot results\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(results['n'], results['sensitivity'], label='Sensitivity')\n",
    "    plt.plot(results['n'], results['specificity'], label='Specificity')\n",
    "    plt.axhline(0.8, color='gray', linestyle='--')\n",
    "\n",
    "    # Add vertical line only if optimal_n exists\n",
    "    if optimal_n is not None:\n",
    "        plt.axvline(optimal_n, color='red', linestyle=':',\n",
    "                    label=f'Optimal N: {optimal_n}')\n",
    "        print(f\"Optimal sample size: {optimal_n}\")\n",
    "    else:\n",
    "        print(\"Warning: No sample size achieved required power\")\n",
    "        plt.axvline((min_n + max_n)//2, color='yellow', linestyle='--',\n",
    "                    label='No optimal N found')\n",
    "\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Diagnostic Classification Performance vs Sample Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return results, optimal_n\n",
    "\n",
    "# Example Usage\n",
    "prior_alpha = 2  # Prior based on normal population (mean=0.2)\n",
    "prior_beta = 8\n",
    "\n",
    "results, optimal_n = diagnostic_sample_size_simulation(\n",
    "    prior_alpha=prior_alpha,\n",
    "    prior_beta=prior_beta,\n",
    "    disease_prev=0.1,   # Disease population prevalence\n",
    "    normal_prev=0.9,   # Normal population prevalence\n",
    "    threshold=1,\n",
    "    min_n=50,\n",
    "    max_n=1000,\n",
    "    step=10\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Optimal sample size: {optimal_n}\")\n"
   ],
   "id": "ae84b6e15567ff0d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
